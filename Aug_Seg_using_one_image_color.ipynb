{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aug_Seg_using_one_image_color.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16c2fc885b084f79b6ff94f7b1fb4a1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3bcf5e41b82048499d60d71870b1772e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_696699e0ad0a45a89cbdcc09afaa5a11",
              "IPY_MODEL_533cf7464dc24b2b9f33949db822e82b"
            ]
          }
        },
        "3bcf5e41b82048499d60d71870b1772e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "696699e0ad0a45a89cbdcc09afaa5a11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_013353a7ac12452784b266bb8ff978e0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c2a74ef88c434cd7bab7bf7482d5aaf8"
          }
        },
        "533cf7464dc24b2b9f33949db822e82b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1e3bdf59b3f44a6fa79798fcc38007c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [16:18&lt;00:00, 105kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4828342ece6147978ded3c42aeeea41b"
          }
        },
        "013353a7ac12452784b266bb8ff978e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c2a74ef88c434cd7bab7bf7482d5aaf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e3bdf59b3f44a6fa79798fcc38007c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4828342ece6147978ded3c42aeeea41b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKyUzRfHzb9h"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import sklearn.svm\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from os.path import exists\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "#cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "#accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "#print(\"Accelerator type = \",accelerator)\n",
        "#print(\"Pytorch verision: \", torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n-_FDq40u0h",
        "outputId": "a29352f4-8292-4e91-f33d-b580b1a49e0d"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_9gKARR0zSI"
      },
      "source": [
        "train_path = '/content/drive/My Drive/cars_latest/train_set/Training_cr.jpg'\n",
        "test_path = '/content/drive/My Drive/Textile/data/test_set'\n",
        "train_label = '/content/drive/My Drive/cars_latest/train_set/DL_cars_csv.csv'\n",
        "test_label = '/content/drive/My Drive/cars_latest/validation_set/DL_cars_valid_csv.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok66dD61KZOE"
      },
      "source": [
        "list_upper = [1,5,7,9,12,13,15,17]\n",
        "list_mid = [2,3,4,6,8,10,11,14,16]\n",
        "list_end = [18,19,20,21,22,23,24,25]\n",
        "list_full = [list_upper,list_mid,list_end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu8vjg-C4xzL"
      },
      "source": [
        "import ast\n",
        "\n",
        "def get_car_labels_coordinates(input_file_path):\n",
        "  label_file = pd.read_csv(input_file_path)\n",
        "\n",
        "  polygon_coord  = label_file['region_shape_attributes']\n",
        "  print(polygon_coord[0])\n",
        "#  for key, value in polygon_coord.items():\n",
        "\n",
        "\n",
        "  x_cord = ast.literal_eval(polygon_coord[0])['all_points_x']\n",
        "  y_cord = ast.literal_eval(polygon_coord[0])['all_points_y']\n",
        "\n",
        "  #print(x_cord)\n",
        "\n",
        "\n",
        "\n",
        "  return polygon_coord"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn0UlZ6KJsNT"
      },
      "source": [
        "def rows_col_pixels(index,rr,cc):\n",
        "  if index+1 in list_upper:\n",
        "    rr_new = rr+ 180\n",
        "    cc_new = cc+ 180\n",
        "  elif index+1 in list_mid:\n",
        "    rr_new = rr+ 100\n",
        "    cc_new = cc+ 100\n",
        "  elif index+1 in list_end:\n",
        "    rr_new = rr- 100\n",
        "    cc_new = cc- 100\n",
        "\n",
        "  return rr_new,cc_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n81CUt9_Bxvt"
      },
      "source": [
        "from skimage.draw import polygon\n",
        "\n",
        "def get_segmentation_mask_single(input_image_path,polygon_coord,index_list):\n",
        "  image_color = cv2.imread(input_image_path)\n",
        "  image_grayscale = cv2.cvtColor(image_color, cv2.COLOR_BGR2GRAY)\n",
        "  image_color_rgb = cv2.cvtColor(image_color, cv2.COLOR_BGR2RGB)\n",
        "  \n",
        "  #plt.imshow(image_color)\n",
        "\n",
        "  rr_list = []\n",
        "  cc_list = []\n",
        "  rr_new_list = []\n",
        "  cc_new_list = []\n",
        "\n",
        "  mask = np.zeros(image_grayscale.shape,dtype=np.uint8)\n",
        "  mask3 = np.zeros(image_grayscale.shape,dtype=np.uint8)\n",
        "  mask2 = image_color_rgb.copy()\n",
        "\n",
        "  for index in index_list:\n",
        "    rr, cc = polygon(np.array(ast.literal_eval(polygon_coord[index])['all_points_y']),np.array(ast.literal_eval(polygon_coord[index])['all_points_x']))\n",
        "    #rr_list.append(rr)\n",
        "    #cc_list.append(cc)\n",
        "    rr_new,cc_new = rows_col_pixels(index,rr,cc)\n",
        "    #rr_new_list.append(rr_new)\n",
        "    #cc_new_list.append(cc_new)\n",
        "\n",
        "    mask2[rr,cc,:] = image_color_rgb[rr_new,cc_new,:]\n",
        "    mask2[rr_new,cc_new,:] = image_color_rgb[rr,cc,:]\n",
        "\n",
        "\n",
        "    mask3[rr_new,cc_new] = 1\n",
        "  '''\n",
        "  plt.figure(1)\n",
        "  plt.imshow(image_grayscale,cmap='gray')\n",
        "  plt.figure(2)\n",
        "  plt.imshow(mask2,cmap='gray')\n",
        "  plt.figure(3)\n",
        "  plt.imshow(mask3,cmap='gray')\n",
        "  '''\n",
        "\n",
        "\n",
        "  return mask2, mask3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLE3Pkhu6l3d"
      },
      "source": [
        "from skimage.draw import polygon\n",
        "\n",
        "def get_segmentation_mask_valid(input_image_path,polygon_coord,index_list):\n",
        "  image_color = cv2.imread(input_image_path)\n",
        "  image_grayscale = cv2.cvtColor(image_color, cv2.COLOR_BGR2GRAY)\n",
        "  image_color_rgb = cv2.cvtColor(image_color, cv2.COLOR_BGR2RGB)\n",
        "  \n",
        "  #plt.imshow(image_color)\n",
        "\n",
        "  rr_list = []\n",
        "  cc_list = []\n",
        "  rr_new_list = []\n",
        "  cc_new_list = []\n",
        "\n",
        "  mask3 = np.zeros(image_grayscale.shape,dtype=np.uint8)\n",
        "  mask2 = image_color_rgb.copy()\n",
        "\n",
        "  for index in index_list:\n",
        "    rr, cc = polygon(np.array(ast.literal_eval(polygon_coord[index])['all_points_y']),np.array(ast.literal_eval(polygon_coord[index])['all_points_x']))\n",
        "\n",
        "    mask2[rr,cc,:] = image_color_rgb[rr,cc,:]\n",
        "\n",
        "    mask3[rr,cc] = 1\n",
        "  '''\n",
        "  plt.figure(1)\n",
        "  plt.imshow(image_grayscale,cmap='gray')\n",
        "  plt.figure(2)\n",
        "  plt.imshow(mask2,cmap='gray')\n",
        "  plt.figure(3)\n",
        "  plt.imshow(mask3,cmap='gray')\n",
        "  '''\n",
        "\n",
        "\n",
        "\n",
        "  return mask2, mask3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc-yb9WhBjAQ"
      },
      "source": [
        "from skimage.draw import polygon\n",
        "\n",
        "def get_segmentation_mask_multiple(input_image_path,polygon_coord,index_list):\n",
        "  image_color = cv2.imread(input_image_path)\n",
        "  image_grayscale = cv2.cvtColor(image_color, cv2.COLOR_BGR2GRAY)\n",
        "  image_color_rgb = cv2.cvtColor(image_color, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  #plt.imshow(image_color)\n",
        "\n",
        "  rr_list = []\n",
        "  cc_list = []\n",
        "  rr_new_list = []\n",
        "  cc_new_list = []\n",
        "\n",
        "  mask = np.zeros(image_grayscale.shape,dtype=np.uint8)\n",
        "  mask3 = np.zeros(image_grayscale.shape,dtype=np.uint8)\n",
        "  mask2 = image_color_rgb.copy()\n",
        "\n",
        "  for index in index_list:\n",
        "    rr, cc = polygon(np.array(ast.literal_eval(polygon_coord[index-1])['all_points_y']),np.array(ast.literal_eval(polygon_coord[index-1])['all_points_x']))\n",
        "    #rr_list.append(rr)\n",
        "    #cc_list.append(cc)\n",
        "    rr_new,cc_new = rows_col_pixels(index-1,rr,cc)\n",
        "    #rr_new_list.append(rr_new)\n",
        "    #cc_new_list.append(cc_new)\n",
        "\n",
        "    mask2[rr,cc,:] = image_color_rgb[rr_new,cc_new,:]\n",
        "    mask2[rr_new,cc_new,:] = image_color_rgb[rr,cc,:]\n",
        "\n",
        "\n",
        "    mask3[rr_new,cc_new] = 1\n",
        "  '''\n",
        "  plt.figure(1)\n",
        "  plt.imshow(image_grayscale,cmap='gray')\n",
        "  plt.figure(2)\n",
        "  plt.imshow(mask2,cmap='gray')\n",
        "  plt.figure(3)\n",
        "  plt.imshow(mask3,cmap='gray')\n",
        "  '''\n",
        "\n",
        "  return mask2, mask3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRqW5cYU1bRN",
        "outputId": "408b7ee6-e33f-4e16-f41e-e8266d525bdb"
      },
      "source": [
        "polygon_coord_train = get_car_labels_coordinates(train_label)\n",
        "polygon_coord_valid = get_car_labels_coordinates(test_label)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"name\":\"polygon\",\"all_points_x\":[340,396,413,436,434,371,349],\"all_points_y\":[38,97,100,76,60,3,15]}\n",
            "{\"name\":\"polygon\",\"all_points_x\":[277,369,371,365,277,272],\"all_points_y\":[98,99,75,54,52,77]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJTCdvPe9uql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbe247f1-bd2c-425f-9ce6-e5eeb1e2d63f"
      },
      "source": [
        "print(len(polygon_coord_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n79lw58dEs0t"
      },
      "source": [
        "mask, mask3= get_segmentation_mask_multiple(train_path,polygon_coord_train,[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_EiDknEnhE1"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from skimage import io, transform,data\n",
        "from torchvision import transforms, utils\n",
        "import numpy as np\n",
        "import math\n",
        "from matplotlib.patches import Ellipse\n",
        "from skimage.draw import ellipse\n",
        "import glob\n",
        "import random\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "class CarData(Dataset):\n",
        "  def __init__(self, root_dir,polygon_coord_train,polygon_coord_valid,train=True,test=False, transform=None,transform_test=None):\n",
        "    self.root_dir = root_dir\n",
        "    self.root_dir_train = root_dir + '/train_set/Training_cr.jpg'\n",
        "    self.root_dir_test = root_dir + '/validation_set/Testset_10m_2.jpg'\n",
        "    self.labels_path = self.root_dir+'/train_set/DL_cars_csv.csv'\n",
        "    self.train = train\n",
        "    self.polygon_coord_train = polygon_coord_train\n",
        "    self.polygon_coord_valid = polygon_coord_valid\n",
        "\n",
        "    #self.transform = transform\n",
        "    self.transform_test = transform_test\n",
        "\n",
        "    self.test = test\n",
        "\n",
        "  def __len__(self):\n",
        "    if self.train==True:\n",
        "      #polygon_coord = get_car_labels_coordinates(self.labels_path)\n",
        "      len1 = 400\n",
        "\n",
        "#      return len(polygon_coord)\n",
        "      return len1\n",
        "\n",
        "    else:\n",
        "\n",
        "      return 1   \n",
        "\n",
        "  def transform(self, image, mask):\n",
        "      #preprocessor = transforms.Compose([transforms.ToTensor(),resize, transforms.RandomHorizontalFlip(),transforms.RandomVerticalFlip(),transforms.RandomRotation(45),transforms.RandomPerspective(), transforms.RandomAffine(degrees=20),normalize])\n",
        "\n",
        "      # Transform to tensor\n",
        "      image = TF.to_tensor(image)\n",
        "      mask = TF.to_tensor(mask)\n",
        "\n",
        "      # Resize\n",
        "      resize = transforms.Resize(size=(256, 256))\n",
        "      image = resize(image)\n",
        "      mask = resize(mask)\n",
        "\n",
        "      # Random horizontal flipping\n",
        "      if random.random() > 0.5:\n",
        "          image = TF.rotate(image,45)\n",
        "          mask = TF.rotate(mask,45)\n",
        "\n",
        "      \n",
        "      # Random horizontal flipping\n",
        "      if random.random() > 0.5:\n",
        "          image = TF.hflip(image)\n",
        "          mask = TF.hflip(mask)\n",
        "\n",
        "      # Random vertical flipping\n",
        "      if random.random() > 0.5:\n",
        "          image = TF.vflip(image)\n",
        "          mask = TF.vflip(mask)\n",
        "      normalize = transforms.Normalize(mean = [0.485,0.456,0.406],\n",
        "                              std = [0.229,0.224,0.225])\n",
        "      \n",
        "\n",
        "      #image = normalize(image)\n",
        "\n",
        "\n",
        "      return image, mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    #index+=1\n",
        "    if torch.is_tensor(index):\n",
        "      index = index.tolist()\n",
        "    #print(index)\n",
        "\n",
        "    if self.train==True:\n",
        "      try:\n",
        "        img_name = self.root_dir_train\n",
        "      except FileNotFoundError:\n",
        "        pass\n",
        "    else:\n",
        "      try:\n",
        "        img_name = self.root_dir_test\n",
        "\n",
        "      except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "    #print(img_name)\n",
        "    #print(ok)\n",
        "    image = io.imread(img_name,as_gray=False)\n",
        "    #print(image.shape)\n",
        "\n",
        "    #print('Index',index)\n",
        "    if self.train==False:\n",
        "      new_image,new_image_mask = get_segmentation_mask_valid(self.root_dir_test,self.polygon_coord_valid,list(range(len(polygon_coord_valid))))\n",
        "\n",
        "      if self.transform_test:\n",
        "        new_image = self.transform_test(new_image)\n",
        "        new_image_mask = self.transform_test(new_image_mask)\n",
        "\n",
        "      sample = {'image': new_image, 'segmented_mask': new_image_mask}\n",
        "\n",
        "      return sample\n",
        "\n",
        "    \n",
        "    if index<25:\n",
        "      new_image,new_image_mask = get_segmentation_mask_single(self.root_dir_train,self.polygon_coord_train,[index])\n",
        "\n",
        "    elif index>24 and index<125:\n",
        "      list_ch = random.sample([0,1,2], 1)\n",
        "      list_select = list_full[list_ch[0]]\n",
        "      random_index1 = random.sample(list_select, 2)\n",
        "      new_image,new_image_mask = get_segmentation_mask_multiple(self.root_dir_train,self.polygon_coord_train,random_index1)\n",
        "    \n",
        "    elif index>124 and index<225:\n",
        "      list_ch = random.sample([0,1,2], 1)\n",
        "      list_select = list_full[list_ch[0]]\n",
        "      random_index1 = random.sample(list_select, 4)\n",
        "      new_image,new_image_mask = get_segmentation_mask_multiple(self.root_dir_train,self.polygon_coord_train,random_index1)\n",
        "\n",
        "    elif index>224 and index<325:\n",
        "      list_ch = random.sample([0,1,2], 1)\n",
        "      list_select = list_full[list_ch[0]]\n",
        "      random_index1 = random.sample(list_select, 6)\n",
        "      new_image,new_image_mask = get_segmentation_mask_multiple(self.root_dir_train,self.polygon_coord_train,random_index1)\n",
        "\n",
        "    elif index>324 and index<401:\n",
        "      list_ch = random.sample([0,1,2], 1)\n",
        "      list_select = list_full[list_ch[0]]\n",
        "      random_index1 = random.sample(list_select, 8)\n",
        "      new_image,new_image_mask = get_segmentation_mask_multiple(self.root_dir_train,self.polygon_coord_train,random_index1)\n",
        "\n",
        "\n",
        "#    if self.transform:\n",
        "  \n",
        "    new_image,new_image_mask = self.transform(new_image,new_image_mask)\n",
        "\n",
        "\n",
        "\n",
        "    sample = {'image': new_image, 'segmented_mask': new_image_mask}\n",
        "\n",
        "    #print(sample['label'])\n",
        "    #plt.imshow(sample['image'])\n",
        "    #plt.show()\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l20QAVJvjFP",
        "outputId": "6ef28cb1-a113-4200-8b46-5ef8d2ff581c"
      },
      "source": [
        "#Pre processing the data\n",
        "normalize = transforms.Normalize(mean = [0.485,0.456,0.406],\n",
        "                                std = [0.229,0.224,0.225])\n",
        "resize = transforms.Resize((256,256))\n",
        "centrecrop = transforms.CenterCrop((224,224))\n",
        "preprocessor = transforms.Compose(\n",
        "    [transforms.ToTensor(),resize, transforms.RandomHorizontalFlip(),transforms.RandomVerticalFlip(),transforms.RandomRotation(45),transforms.RandomPerspective(), transforms.RandomAffine(degrees=20),normalize])\n",
        "#minority_preprocessor = transforms.Compose([ transforms.ToTensor(),resize,transforms.RandomHorizontalFlip(), transforms.RandomRotation(45),transforms.RandomVerticalFlip(), transforms.RandomPerspective(), transforms.RandomAffine(degrees=20), transforms.RandomResizedCrop(224),normalize\n",
        "#                                   ])\n",
        "\n",
        "minority_preprocessor = transforms.Compose([ transforms.ToTensor(),resize, transforms.RandomHorizontalFlip(),transforms.RandomVerticalFlip(),transforms.RandomRotation(45),transforms.RandomPerspective(), transforms.RandomAffine(degrees=20),normalize\n",
        "                                   ])\n",
        "\n",
        "\n",
        "#preprocessor = transforms.Compose(\n",
        "#    [transforms.ToTensor(),resize, transforms.RandomHorizontalFlip(),transforms.RandomVerticalFlip(),transforms.RandomRotation(45),transforms.RandomPerspective(), transforms.RandomAffine(degrees=20)])\n",
        "\n",
        "preprocessor_test = transforms.Compose([ transforms.ToTensor(),resize\n",
        "                                   ])\n",
        "\n",
        "print(preprocessor.transforms[-1])\n",
        "\n",
        "train_dataset_full = CarData(root_dir='/content/drive/My Drive/cars_latest',polygon_coord_train=polygon_coord_train,polygon_coord_valid=polygon_coord_valid, train=True,test=False, transform=preprocessor)\n",
        "test_dataset_full = CarData(root_dir='/content/drive/My Drive/cars_latest',polygon_coord_train=polygon_coord_train,polygon_coord_valid=polygon_coord_valid, train=False,test=False, transform=preprocessor,transform_test=preprocessor_test)\n",
        "'''\n",
        "print(len(train_dataset_full))\n",
        "\n",
        "train_split = 0.8\n",
        "dataset_size  = len(train_dataset_full)\n",
        "indices = list(range(dataset_size))\n",
        "np.random.shuffle(indices) # shuffle the dataset before splitting into train and val\n",
        "split = int(np.floor(train_split * dataset_size))\n",
        "\n",
        "print(split)\n",
        "\n",
        "#train_dataset, test_dataset = torch.utils.data.random_split(train_dataset_full, [train_size, test_size])\n",
        "\n",
        "train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "\n",
        "trainset = torch.utils.data.Subset(train_dataset_full, train_indices)\n",
        "testset = torch.utils.data.Subset(train_dataset_full, val_indices)\n",
        "\n",
        "print(len(train_indices))\n",
        "print(len(val_indices))\n",
        "'''\n",
        "trainloader = DataLoader(train_dataset_full, batch_size=8, shuffle=True)\n",
        "testloader = DataLoader(test_dataset_full,batch_size=1, shuffle=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aKfaIpFB-t5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2cd5541-a267-49f5-a39e-17935b9eed7b"
      },
      "source": [
        "train_dataset_full[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': tensor([[[6.0137e-01, 5.8349e-01, 5.8837e-01,  ..., 5.5987e-01,\n",
              "           5.2785e-01, 3.5530e-01],\n",
              "          [6.0331e-01, 5.8004e-01, 5.8848e-01,  ..., 5.4655e-01,\n",
              "           4.4296e-01, 2.7946e-01],\n",
              "          [5.9654e-01, 5.7552e-01, 5.8851e-01,  ..., 4.5323e-01,\n",
              "           3.2158e-01, 2.1308e-01],\n",
              "          ...,\n",
              "          [5.3838e-01, 5.9370e-01, 5.1150e-01,  ..., 4.5945e-01,\n",
              "           5.1726e-01, 1.1449e-03],\n",
              "          [5.4634e-01, 6.9113e-01, 5.4217e-01,  ..., 4.6661e-01,\n",
              "           5.6570e-01, 2.0234e-03],\n",
              "          [5.6815e-01, 7.0554e-01, 6.3277e-01,  ..., 5.2396e-01,\n",
              "           5.5841e-01, 0.0000e+00]],\n",
              " \n",
              "         [[6.0529e-01, 5.8741e-01, 5.9229e-01,  ..., 5.8339e-01,\n",
              "           5.7491e-01, 4.2034e-01],\n",
              "          [6.0723e-01, 5.8396e-01, 5.9240e-01,  ..., 5.7789e-01,\n",
              "           4.9394e-01, 3.3706e-01],\n",
              "          [6.0046e-01, 5.7944e-01, 5.9243e-01,  ..., 4.8468e-01,\n",
              "           3.6472e-01, 2.6406e-01],\n",
              "          ...,\n",
              "          [6.1177e-01, 6.7084e-01, 5.8146e-01,  ..., 4.7750e-01,\n",
              "           5.2903e-01, 1.9568e-03],\n",
              "          [5.9894e-01, 7.5780e-01, 6.2816e-01,  ..., 4.8229e-01,\n",
              "           5.7747e-01, 2.8353e-03],\n",
              "          [6.2075e-01, 7.7221e-01, 7.1877e-01,  ..., 5.3964e-01,\n",
              "           5.7018e-01, 7.4529e-04]],\n",
              " \n",
              "         [[6.2490e-01, 6.0702e-01, 6.1190e-01,  ..., 6.6967e-01,\n",
              "           6.6118e-01, 4.9958e-01],\n",
              "          [6.2684e-01, 6.0357e-01, 6.1201e-01,  ..., 6.5240e-01,\n",
              "           5.6845e-01, 4.1818e-01],\n",
              "          [6.2007e-01, 5.9905e-01, 6.1204e-01,  ..., 5.5903e-01,\n",
              "           4.4027e-01, 3.3857e-01],\n",
              "          ...,\n",
              "          [6.7139e-01, 7.2878e-01, 6.4238e-01,  ..., 5.0569e-01,\n",
              "           5.5648e-01, 1.8455e-02],\n",
              "          [6.7345e-01, 8.2055e-01, 6.7130e-01,  ..., 5.2543e-01,\n",
              "           6.0492e-01, 1.3114e-02],\n",
              "          [6.9526e-01, 8.3496e-01, 7.6190e-01,  ..., 5.8278e-01,\n",
              "           5.9763e-01, 1.1024e-02]]]),\n",
              " 'segmented_mask': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc3dhlW8v34K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "16c2fc885b084f79b6ff94f7b1fb4a1f",
            "3bcf5e41b82048499d60d71870b1772e",
            "696699e0ad0a45a89cbdcc09afaa5a11",
            "533cf7464dc24b2b9f33949db822e82b",
            "013353a7ac12452784b266bb8ff978e0",
            "c2a74ef88c434cd7bab7bf7482d5aaf8",
            "1e3bdf59b3f44a6fa79798fcc38007c9",
            "4828342ece6147978ded3c42aeeea41b"
          ]
        },
        "outputId": "1987ed22-1a84-40c4-d838-54f11407f5c2"
      },
      "source": [
        "\n",
        "import torchvision\n",
        "resnet = torchvision.models.resnet.resnet50(pretrained=True)\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Helper module that consists of a Conv -> BN -> ReLU\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, padding=1, kernel_size=3, stride=1, with_nonlinearity=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, padding=padding, kernel_size=kernel_size, stride=stride)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.with_nonlinearity = with_nonlinearity\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        if self.with_nonlinearity:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Bridge(nn.Module):\n",
        "    \"\"\"\n",
        "    This is the middle layer of the UNet which just consists of some\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.bridge = nn.Sequential(\n",
        "            ConvBlock(in_channels, out_channels),\n",
        "            ConvBlock(out_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bridge(x)\n",
        "\n",
        "\n",
        "class UpBlockForUNetWithResNet50(nn.Module):\n",
        "    \"\"\"\n",
        "    Up block that encapsulates one up-sampling step which consists of Upsample -> ConvBlock -> ConvBlock\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, up_conv_in_channels=None, up_conv_out_channels=None,\n",
        "                 upsampling_method=\"conv_transpose\"):\n",
        "        super().__init__()\n",
        "\n",
        "        if up_conv_in_channels == None:\n",
        "            up_conv_in_channels = in_channels\n",
        "        if up_conv_out_channels == None:\n",
        "            up_conv_out_channels = out_channels\n",
        "\n",
        "        if upsampling_method == \"conv_transpose\":\n",
        "            self.upsample = nn.ConvTranspose2d(up_conv_in_channels, up_conv_out_channels, kernel_size=2, stride=2)\n",
        "        elif upsampling_method == \"bilinear\":\n",
        "            self.upsample = nn.Sequential(\n",
        "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
        "            )\n",
        "        self.conv_block_1 = ConvBlock(in_channels, out_channels)\n",
        "        self.conv_block_2 = ConvBlock(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, up_x, down_x):\n",
        "        \"\"\"\n",
        "        :param up_x: this is the output from the previous up block\n",
        "        :param down_x: this is the output from the down block\n",
        "        :return: upsampled feature map\n",
        "        \"\"\"\n",
        "        x = self.upsample(up_x)\n",
        "        x = torch.cat([x, down_x], 1)\n",
        "        x = self.conv_block_1(x)\n",
        "        x = self.conv_block_2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNetWithResnet50Encoder(nn.Module):\n",
        "    DEPTH = 6\n",
        "\n",
        "    def __init__(self, n_classes=2):\n",
        "        super().__init__()\n",
        "        resnet = torchvision.models.resnet.resnet50(pretrained=True)\n",
        "        down_blocks = []\n",
        "        up_blocks = []\n",
        "        self.input_block = nn.Sequential(*list(resnet.children()))[:3]\n",
        "        self.input_pool = list(resnet.children())[3]\n",
        "        for bottleneck in list(resnet.children()):\n",
        "            if isinstance(bottleneck, nn.Sequential):\n",
        "                down_blocks.append(bottleneck)\n",
        "        self.down_blocks = nn.ModuleList(down_blocks)\n",
        "        self.bridge = Bridge(2048, 2048)\n",
        "        up_blocks.append(UpBlockForUNetWithResNet50(2048, 1024))\n",
        "        up_blocks.append(UpBlockForUNetWithResNet50(1024, 512))\n",
        "        up_blocks.append(UpBlockForUNetWithResNet50(512, 256))\n",
        "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=128 + 64, out_channels=128,\n",
        "                                                    up_conv_in_channels=256, up_conv_out_channels=128))\n",
        "        up_blocks.append(UpBlockForUNetWithResNet50(in_channels=64 + 3, out_channels=64,\n",
        "                                                    up_conv_in_channels=128, up_conv_out_channels=64))\n",
        "\n",
        "        self.up_blocks = nn.ModuleList(up_blocks)\n",
        "\n",
        "        self.out = nn.Conv2d(64, n_classes, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x, with_output_feature_map=False):\n",
        "        pre_pools = dict()\n",
        "        pre_pools[f\"layer_0\"] = x\n",
        "        x = self.input_block(x)\n",
        "        pre_pools[f\"layer_1\"] = x\n",
        "        x = self.input_pool(x)\n",
        "\n",
        "        for i, block in enumerate(self.down_blocks, 2):\n",
        "            x = block(x)\n",
        "            if i == (UNetWithResnet50Encoder.DEPTH - 1):\n",
        "                continue\n",
        "            pre_pools[f\"layer_{i}\"] = x\n",
        "\n",
        "        x = self.bridge(x)\n",
        "\n",
        "        for i, block in enumerate(self.up_blocks, 1):\n",
        "            key = f\"layer_{UNetWithResnet50Encoder.DEPTH - 1 - i}\"\n",
        "            x = block(x, pre_pools[key])\n",
        "        output_feature_map = x\n",
        "        x = self.out(x)\n",
        "        del pre_pools\n",
        "        if with_output_feature_map:\n",
        "            return x, output_feature_map\n",
        "        else:\n",
        "            return x\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16c2fc885b084f79b6ff94f7b1fb4a1f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H-lcRMOf4Lb"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWeM_kqFfyCN"
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        #print(x.shape)\n",
        "        logits = self.outc(x)\n",
        "        #print(logits.shape)\n",
        "        #print(ok)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78rJfcom3jDS"
      },
      "source": [
        "class IoULoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(IoULoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        \n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        inputs = F.sigmoid(inputs)       \n",
        "        \n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "        \n",
        "        #intersection is equivalent to True Positive count\n",
        "        #union is the mutually inclusive area of all labels & predictions \n",
        "        intersection = (inputs * targets).sum()\n",
        "        total = (inputs + targets).sum()\n",
        "        union = total - intersection \n",
        "        \n",
        "        IoU = (intersection + smooth)/(union + smooth)\n",
        "                \n",
        "        return 1 - IoU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6-6IGRg3lbn"
      },
      "source": [
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "\n",
        "def dice_loss(pred, target, smooth = 1.):\n",
        "    pred = pred.contiguous()\n",
        "    target = target.contiguous()    \n",
        "\n",
        "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
        "    \n",
        "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
        "    \n",
        "    return loss.mean()\n",
        "\n",
        "smooth = 1e-12\n",
        "def jaccard_approx(pred, target, smooth=1e-12 ):\n",
        "    #intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
        "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
        "    #sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
        "    sum_ = (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2))\n",
        "\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "\n",
        "    return jac.mean()\n",
        "\n",
        "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
        "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
        "\n",
        "    pred = F.sigmoid(pred)\n",
        "\n",
        "    #dice = dice_loss(pred, target)\n",
        "    #loss = bce * bce_weight + dice * (1 - bce_weight)\n",
        "    #jaccloss = torch.log(jaccard_approx(pred,target))\n",
        "    loss = bce\n",
        "\n",
        "    #iou_loss = IoULoss()\n",
        "    #loss = iou_loss.forward(pred,target)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
        "    #metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
        "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
        "    #metrics['iouloss'] += iouloss.data.cpu().numpy() * target.size(0)\n",
        "    #metrics['jaccloss'] += jaccloss.data.cpu().numpy() * target.size(0)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def print_metrics(metrics, epoch_samples, phase):\n",
        "    outputs = []\n",
        "    for k in metrics.keys():\n",
        "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
        "\n",
        "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
        "\n",
        "\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "\n",
        "def train_model(model, optimizer, scheduler, num_epochs=10):\n",
        "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        since = time.time()\n",
        "\n",
        "      # Each epoch has a training and validation phase\n",
        "\n",
        "        scheduler.step()\n",
        "        for param_group in optimizer.param_groups:\n",
        "            print(\"LR\", param_group['lr'])\n",
        "        print('Started training')\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "\n",
        "        metrics = defaultdict(float)\n",
        "        epoch_samples = 0\n",
        "\n",
        "        for inputs in trainloader:\n",
        "            #print(dataloaders['train'])\n",
        "            #print('Entered')\n",
        "            inputs['image'] = inputs['image'].to(device)\n",
        "            inputs['segmented_mask'] =  inputs['segmented_mask'].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "\n",
        "            outputs = model(inputs['image'])\n",
        "            loss = calc_loss(outputs, inputs['segmented_mask'], metrics)\n",
        "\n",
        "            # backward + optimize only if in training phase\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            epoch_samples += inputs['image'].size(0)\n",
        "\n",
        "        print_metrics(metrics, epoch_samples, 'train')\n",
        "        epoch_loss = metrics['loss'] / epoch_samples\n",
        "        train_loss.append(epoch_loss)\n",
        "\n",
        "        #torch.save(model,f'/content/drive/My Drive/cars_latest/resunet50_epoch_{e}_1.pt')\n",
        "\n",
        "\n",
        "        #set model in evaluation mode\n",
        "        model.eval()\n",
        "        avg_loss = 0\n",
        "\n",
        "        metrics = defaultdict(float)\n",
        "        epoch_samples=0\n",
        "        \n",
        "        for inputs in testloader:\n",
        "\n",
        "            #print(dataloaders['train'])\n",
        "            inputs['image'] = inputs['image'].to(device)\n",
        "            #print(inputs['image'].shape)\n",
        "            inputs['segmented_mask'] =  inputs['segmented_mask'].to(device)\n",
        "            \n",
        "\n",
        "            outputs = model(inputs['image'])\n",
        "            loss = calc_loss(outputs, inputs['segmented_mask'], metrics)\n",
        "            epoch_samples += inputs['image'].size(0)\n",
        "\n",
        "        \n",
        "        print_metrics(metrics, epoch_samples, 'val')\n",
        "        epoch_loss = metrics['loss'] / epoch_samples\n",
        "        test_loss.append(epoch_loss)\n",
        "        #print(\"\\n\")\n",
        "        print(\"Epoch: \", epoch, \"Train Loss: \", train_loss[-1], \"Test Loss: \", test_loss[-1])\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        print(\"\\n\")\n",
        "\n",
        "        torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'train_loss': train_loss[-1],\n",
        "              }, f'/content/drive/My Drive/cars_latest/epoch_{epoch}_trainloss_{train_loss[-1]}_testloss_{test_loss[-1]}_resunet50_3.pt')\n",
        "\n",
        "\n",
        "    #print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    #model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVjnqclU1i0H",
        "outputId": "69dc9a44-1c1e-4dc4-a3c8-ff887119e689"
      },
      "source": [
        "# define loss function\n",
        "#model = UNetWithResnet50Encoder(n_classes=1)\n",
        "model = UNet(n_channels=3, n_classes=1)\n",
        "\n",
        "# setup SGD\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "gpu_flag = torch.cuda.is_available()\n",
        "print(gpu_flag)\n",
        "if gpu_flag:\n",
        "    model = model.cuda()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWI0rC-e3_5Z",
        "outputId": "94d1d302-e9e9-46d2-c904-3503af36cfe8"
      },
      "source": [
        "model_trained = train_model(model, optimizer, exp_lr_scheduler, num_epochs=50)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/49\n",
            "----------\n",
            "LR 0.001\n",
            "Started training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train: loss: 0.298264\n",
            "val: loss: 0.177500\n",
            "Epoch:  0 Train Loss:  0.29826408326625825 Test Loss:  0.17749997973442078\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "LR 0.001\n",
            "Started training\n",
            "train: loss: 0.097128\n",
            "val: loss: 0.066331\n",
            "Epoch:  1 Train Loss:  0.09712825745344161 Test Loss:  0.0663306713104248\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "LR 0.001\n",
            "Started training\n",
            "train: loss: 0.048497\n",
            "val: loss: 0.036388\n",
            "Epoch:  2 Train Loss:  0.048496567159891125 Test Loss:  0.036388251930475235\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "LR 0.001\n",
            "Started training\n",
            "train: loss: 0.029001\n",
            "val: loss: 0.024970\n",
            "Epoch:  3 Train Loss:  0.02900135338306427 Test Loss:  0.024970097467303276\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "LR 0.001\n",
            "Started training\n",
            "train: loss: 0.018716\n",
            "val: loss: 0.018852\n",
            "Epoch:  4 Train Loss:  0.01871644616127014 Test Loss:  0.01885201968252659\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "LR 0.001\n",
            "Started training\n",
            "train: loss: 0.013231\n",
            "val: loss: 0.015146\n",
            "Epoch:  5 Train Loss:  0.01323089910671115 Test Loss:  0.015146438963711262\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "LR 0.0001\n",
            "Started training\n",
            "train: loss: 0.011165\n",
            "val: loss: 0.014812\n",
            "Epoch:  6 Train Loss:  0.011164623387157917 Test Loss:  0.014812428504228592\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "LR 0.0001\n",
            "Started training\n",
            "train: loss: 0.010856\n",
            "val: loss: 0.014536\n",
            "Epoch:  7 Train Loss:  0.010856081563979388 Test Loss:  0.014535668306052685\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "LR 0.0001\n",
            "Started training\n",
            "train: loss: 0.010530\n",
            "val: loss: 0.014296\n",
            "Epoch:  8 Train Loss:  0.01052982395514846 Test Loss:  0.014295621775090694\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "LR 0.0001\n",
            "Started training\n",
            "train: loss: 0.010216\n",
            "val: loss: 0.013984\n",
            "Epoch:  9 Train Loss:  0.010215773358941077 Test Loss:  0.01398414559662342\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "LR 0.0001\n",
            "Started training\n",
            "train: loss: 0.009900\n",
            "val: loss: 0.013772\n",
            "Epoch:  10 Train Loss:  0.009900324195623398 Test Loss:  0.013771526515483856\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "LR 0.0001\n",
            "Started training\n",
            "train: loss: 0.009582\n",
            "val: loss: 0.013536\n",
            "Epoch:  11 Train Loss:  0.009582017790526151 Test Loss:  0.013536014594137669\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "LR 0.0001\n",
            "Started training\n",
            "train: loss: 0.009276\n",
            "val: loss: 0.013305\n",
            "Epoch:  12 Train Loss:  0.009275547694414854 Test Loss:  0.013304679654538631\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "LR 1e-05\n",
            "Started training\n",
            "train: loss: 0.009096\n",
            "val: loss: 0.013327\n",
            "Epoch:  13 Train Loss:  0.009095861874520779 Test Loss:  0.01332743652164936\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "LR 1e-05\n",
            "Started training\n",
            "train: loss: 0.009065\n",
            "val: loss: 0.013248\n",
            "Epoch:  14 Train Loss:  0.00906478701159358 Test Loss:  0.013247981667518616\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "LR 1e-05\n",
            "Started training\n",
            "train: loss: 0.009034\n",
            "val: loss: 0.013219\n",
            "Epoch:  15 Train Loss:  0.009033700600266457 Test Loss:  0.013219273649156094\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "LR 1e-05\n",
            "Started training\n",
            "train: loss: 0.009003\n",
            "val: loss: 0.013109\n",
            "Epoch:  16 Train Loss:  0.009002717211842537 Test Loss:  0.013109448365867138\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "LR 1e-05\n",
            "Started training\n",
            "train: loss: 0.008959\n",
            "val: loss: 0.013168\n",
            "Epoch:  17 Train Loss:  0.008959283195436001 Test Loss:  0.013167733326554298\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "LR 1e-05\n",
            "Started training\n",
            "train: loss: 0.008919\n",
            "val: loss: 0.013083\n",
            "Epoch:  18 Train Loss:  0.008918791711330414 Test Loss:  0.013083299621939659\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "LR 1e-05\n",
            "Started training\n",
            "train: loss: 0.008885\n",
            "val: loss: 0.013136\n",
            "Epoch:  19 Train Loss:  0.008884750995784998 Test Loss:  0.013135738670825958\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "Started training\n",
            "train: loss: 0.008863\n",
            "val: loss: 0.013130\n",
            "Epoch:  20 Train Loss:  0.008862883001565933 Test Loss:  0.013129614293575287\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "Started training\n",
            "train: loss: 0.008850\n",
            "val: loss: 0.013041\n",
            "Epoch:  21 Train Loss:  0.008849624637514352 Test Loss:  0.013041291385889053\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "Started training\n",
            "train: loss: 0.008849\n",
            "val: loss: 0.013102\n",
            "Epoch:  22 Train Loss:  0.008849406745284796 Test Loss:  0.013102379627525806\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "Started training\n",
            "train: loss: 0.008836\n",
            "val: loss: 0.013039\n",
            "Epoch:  23 Train Loss:  0.008836308401077986 Test Loss:  0.013039054349064827\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "Started training\n",
            "train: loss: 0.008833\n",
            "val: loss: 0.013046\n",
            "Epoch:  24 Train Loss:  0.008833153303712607 Test Loss:  0.013046147301793098\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "Started training\n",
            "train: loss: 0.008831\n",
            "val: loss: 0.013036\n",
            "Epoch:  25 Train Loss:  0.008831491693854332 Test Loss:  0.013036368414759636\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "LR 1.0000000000000002e-06\n",
            "Started training\n",
            "train: loss: 0.008824\n",
            "val: loss: 0.013031\n",
            "Epoch:  26 Train Loss:  0.008823818080127239 Test Loss:  0.013030678033828735\n",
            "0m 24s\n",
            "\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "LR 1.0000000000000002e-07\n",
            "Started training\n",
            "train: loss: 0.008814\n",
            "val: loss: 0.013048\n",
            "Epoch:  27 Train Loss:  0.00881404209882021 Test Loss:  0.013048102147877216\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "LR 1.0000000000000002e-07\n",
            "Started training\n",
            "train: loss: 0.008815\n",
            "val: loss: 0.013025\n",
            "Epoch:  28 Train Loss:  0.00881515733897686 Test Loss:  0.013025468215346336\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "LR 1.0000000000000002e-07\n",
            "Started training\n",
            "train: loss: 0.008816\n",
            "val: loss: 0.013039\n",
            "Epoch:  29 Train Loss:  0.008815607000142335 Test Loss:  0.013038854114711285\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "LR 1.0000000000000002e-07\n",
            "Started training\n",
            "train: loss: 0.008824\n",
            "val: loss: 0.013085\n",
            "Epoch:  30 Train Loss:  0.008824166674166918 Test Loss:  0.013084868900477886\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "LR 1.0000000000000002e-07\n",
            "Started training\n",
            "train: loss: 0.008820\n",
            "val: loss: 0.013096\n",
            "Epoch:  31 Train Loss:  0.008820475805550813 Test Loss:  0.01309623010456562\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "LR 1.0000000000000002e-07\n",
            "Started training\n",
            "train: loss: 0.008825\n",
            "val: loss: 0.013100\n",
            "Epoch:  32 Train Loss:  0.00882453415542841 Test Loss:  0.013099690899252892\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "LR 1.0000000000000002e-07\n",
            "Started training\n",
            "train: loss: 0.008820\n",
            "val: loss: 0.013097\n",
            "Epoch:  33 Train Loss:  0.008819947279989719 Test Loss:  0.013096596114337444\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "LR 1.0000000000000004e-08\n",
            "Started training\n",
            "train: loss: 0.008819\n",
            "val: loss: 0.013049\n",
            "Epoch:  34 Train Loss:  0.00881917506456375 Test Loss:  0.013048981316387653\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "LR 1.0000000000000004e-08\n",
            "Started training\n",
            "train: loss: 0.008824\n",
            "val: loss: 0.013037\n",
            "Epoch:  35 Train Loss:  0.008824356943368911 Test Loss:  0.013036874122917652\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "LR 1.0000000000000004e-08\n",
            "Started training\n",
            "train: loss: 0.008812\n",
            "val: loss: 0.013066\n",
            "Epoch:  36 Train Loss:  0.008812492582947015 Test Loss:  0.013065626844763756\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "LR 1.0000000000000004e-08\n",
            "Started training\n",
            "train: loss: 0.008814\n",
            "val: loss: 0.013046\n",
            "Epoch:  37 Train Loss:  0.00881354784592986 Test Loss:  0.013045993633568287\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "LR 1.0000000000000004e-08\n",
            "Started training\n",
            "train: loss: 0.008829\n",
            "val: loss: 0.013038\n",
            "Epoch:  38 Train Loss:  0.008828905578702688 Test Loss:  0.01303815096616745\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "LR 1.0000000000000004e-08\n",
            "Started training\n",
            "train: loss: 0.008811\n",
            "val: loss: 0.013044\n",
            "Epoch:  39 Train Loss:  0.008810980189591646 Test Loss:  0.01304394006729126\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "LR 1.0000000000000004e-08\n",
            "Started training\n",
            "train: loss: 0.008825\n",
            "val: loss: 0.013045\n",
            "Epoch:  40 Train Loss:  0.008824868593364954 Test Loss:  0.013044539839029312\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "LR 1.0000000000000005e-09\n",
            "Started training\n",
            "train: loss: 0.008815\n",
            "val: loss: 0.013045\n",
            "Epoch:  41 Train Loss:  0.008815043326467275 Test Loss:  0.013045246712863445\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "LR 1.0000000000000005e-09\n",
            "Started training\n",
            "train: loss: 0.008814\n",
            "val: loss: 0.013033\n",
            "Epoch:  42 Train Loss:  0.00881419388577342 Test Loss:  0.013033299706876278\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "LR 1.0000000000000005e-09\n",
            "Started training\n",
            "train: loss: 0.008818\n",
            "val: loss: 0.013048\n",
            "Epoch:  43 Train Loss:  0.00881823994219303 Test Loss:  0.013047656044363976\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "LR 1.0000000000000005e-09\n",
            "Started training\n",
            "train: loss: 0.008806\n",
            "val: loss: 0.013030\n",
            "Epoch:  44 Train Loss:  0.008806221764534711 Test Loss:  0.013030032627284527\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "LR 1.0000000000000005e-09\n",
            "Started training\n",
            "train: loss: 0.008813\n",
            "val: loss: 0.013052\n",
            "Epoch:  45 Train Loss:  0.008812647294253111 Test Loss:  0.013051813468337059\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "LR 1.0000000000000005e-09\n",
            "Started training\n",
            "train: loss: 0.008808\n",
            "val: loss: 0.013070\n",
            "Epoch:  46 Train Loss:  0.008807702157646417 Test Loss:  0.01306985318660736\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "LR 1.0000000000000005e-09\n",
            "Started training\n",
            "train: loss: 0.008815\n",
            "val: loss: 0.013056\n",
            "Epoch:  47 Train Loss:  0.008815453536808491 Test Loss:  0.013056036084890366\n",
            "0m 23s\n",
            "\n",
            "\n",
            "Epoch 48/49\n",
            "----------\n",
            "LR 1.0000000000000006e-10\n",
            "Started training\n",
            "train: loss: 0.008807\n",
            "val: loss: 0.013002\n",
            "Epoch:  48 Train Loss:  0.008806776851415635 Test Loss:  0.013002386316657066\n",
            "0m 22s\n",
            "\n",
            "\n",
            "Epoch 49/49\n",
            "----------\n",
            "LR 1.0000000000000006e-10\n",
            "Started training\n",
            "train: loss: 0.008817\n",
            "val: loss: 0.013027\n",
            "Epoch:  49 Train Loss:  0.008817425146698952 Test Loss:  0.013027088716626167\n",
            "0m 22s\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESIafdOW_TYX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}